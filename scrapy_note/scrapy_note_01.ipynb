{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapy Note まとめ\n",
    "\n",
    "<https://sugiaki1989.gitbook.io/scrapy-note/>\n",
    "\n",
    "- Reference\n",
    "  - <https://scrapy-docs-ja.readthedocs.io/ja/latest/index.html>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## インストール\n",
    "\n",
    "```bash\n",
    "pip install Scrapy\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## プロジェクトの作成\n",
    "\n",
    "```bash\n",
    "# scrapy startproject <project-name>\n",
    "scrapy startproject sample_project\n",
    "```\n",
    "\n",
    "```bash\n",
    "# ディレクトリの移動\n",
    "cd sample_project\n",
    "```\n",
    "\n",
    "```bash\n",
    "# クローラー名（spider名）とスクレイピング対象のURLを指定\n",
    "# scrapy genspider <spider-name> <page-url>\n",
    "scrapy genspider sample example.com\n",
    "```\n",
    "\n",
    "```bash\n",
    "# ディレクトリの表示\n",
    "tree\n",
    "\n",
    ".\n",
    "├── scrapy.cfg\n",
    "└── sample_project\n",
    "    ├── __init__.py\n",
    "    ├── items.py\n",
    "    ├── middlewares.py\n",
    "    ├── pipelines.py\n",
    "    ├── settings.py\n",
    "    └── spiders\n",
    "        ├── __init__.py\n",
    "        └── sample.py\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Scrapy\n",
    "  - 収集するデータを定義する\n",
    "  - 抽出する場所と範囲を決定する\n",
    "  - 抽出するための方法を記述する\n",
    "  - 抽出したアイテムを処理する方法を決定する\n",
    "  - クローラーを起動し、スクレイピングを実行する"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- items.py\n",
    "  - 出力されるデータの形式を定義して、構造化されたデータを格納する\n",
    "  - Itemクラス\n",
    "  - Itemオブジェクト\n",
    "\n",
    "```python\n",
    "# items.py\n",
    "class Product(scrapy.Item):\n",
    "    X = scrapy.Field()\n",
    "    Y = scrapy.Field()\n",
    "    Z = scrapy.Field()\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- spiders/spider.py (sample.py)\n",
    "  - `scrapy genspider`コマンドで生成したspidersディレクトリ内の実行ファイル。\n",
    "  - クローラーの動きやスクレイピング方法を定義するクラス。\n",
    "- QuotesSpider\n",
    "  - allowed_domainsで許可されたドメイン内において、\n",
    "  - start_urlsで指定されたURLを起点にQuotesSpiderは動き始める。\n",
    "  - URLのRequestを生成するstart_requestsメソッドとRequestのコールバック関数としてparseメソッドを呼び出す。\n",
    "  - コールバック関数は、\n",
    "    - ResponseであるWebページをセレクターを使用してページ内容をパースして、\n",
    "    - データを抽出を行い、\n",
    "    - 辞書形式でItemを返す。\n",
    "  - Itemは、データベースやCSV、JSONに保存できる。\n",
    "\n",
    "```python\n",
    "# spiders/spider.py (sample.py)\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = 'quotes'\n",
    "    allowed_domains = ['quotes.toscrape.com']\n",
    "    start_urls = ['http://quotes.toscrape.com/']\n",
    "\n",
    "    def parse(self, response):\n",
    "        quotes = response.xpath('//*[@class=\"quote\"]')\n",
    "        for quote in quotes:\n",
    "            text = quote.xpath('.//*[@class=\"text\"]/text()').get()\n",
    "            author = quote.xpath('.//*[@class=\"author\"]/text()').get()\n",
    "            tags = quote.xpath('.//*[@class=\"tag\"]/text()').getall()\n",
    "\n",
    "            yield {\n",
    "                \"text\": text,\n",
    "                \"author\": author,\n",
    "                \"tags\": tags\n",
    "            }\n",
    "\n",
    "        next_page_url = response.xpath('//*[@class=\"next\"]/a/@href').get()\n",
    "        abs_next_page_url = response.urljoin(next_page_url)\n",
    "        if next_page_url is not None:\n",
    "            yield scrapy.Request(abs_next_page_url, callback=self.parse)\n",
    "```\n",
    "\n",
    "- name:\n",
    "  - ターミナルでクローラーを起動させる場合に必要なクローラー名。\n",
    "- allowed_domains:\n",
    "  - クロールできるドメイン名。\n",
    "- start_urls:\n",
    "  - クロールを開始するURLのリスト。\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "```bash\n",
    "# scrapy crawl <crawler-name> <option> <filename>\n",
    "scrapy crawl quotes -o output.json\n",
    "```\n",
    "\n",
    "- 上記スクリプトが実行されると、scrapyが動く。\n",
    "- `-o`: アウトプットする形式を選べる。\n",
    "  - XML, CSV, JSON, JSON-LINES。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pipelines.py\n",
    "  - SPIDERによってスクレイピングされたItemは、ITEM-PIPELINESに送られて処理される。\n",
    "  - データのバリデーション\n",
    "  - データクリーニング\n",
    "  - 画像のサイズ変更、その他の画像処理\n",
    "  - データベースへの保存\n",
    "\n",
    "```python\n",
    "# pipeline.py\n",
    "# 特定の条件に当てはまるものを削除する\n",
    "from scrapy.exceptions import DropItem\n",
    "\n",
    "class PricePipeline(object):\n",
    "\n",
    "    vat_factor = 1.15\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        if item.get('price'):\n",
    "            if item.get('price_excludes_vat'):\n",
    "                item['price'] = item['price'] * self.vat_factor\n",
    "            return item\n",
    "        else:\n",
    "            raise DropItem(\"Missing price in %s\" % item)\n",
    "```\n",
    "\n",
    "- ITEM-PIPELINES コンポーネントをアクティブにするために、setting.pyに設定を追加する。\n",
    "\n",
    "```python\n",
    "# setting.py\n",
    "ITEM_PIPELINES = {\n",
    "    'myproject.pipelines.PricePipeline': 300\n",
    "}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- middlewares.py\n",
    "  - Scrapyを拡張させるために使用される機能。\n",
    "  - Downloader Middleware:\n",
    "    - Webページのダウンロード処理を拡張する。\n",
    "    - <https://scrapy-docs-ja.readthedocs.io/ja/latest/topics/downloader-middleware.html>\n",
    "  - Spider Middleware:\n",
    "    - コールバック関数の処理を拡張する。\n",
    "    - <https://scrapy-docs-ja.readthedocs.io/ja/latest/topics/spider-middleware.html>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Scrapy Architecture\n",
    "\n",
    "![](scrapy_architecture_02.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1.Scrapyエンジンは、Spidersからクロールする最初のRequestsを取得。\n",
    "- 2.Scrapyエンジンは、SchedulerでRequestsをスケジュールリングし、クロールする次のRequestsを求める。\n",
    "- 3.Schedulerは、RequestsをScrapyエンジンに返す。\n",
    "- 4.Scrapyエンジンは、DownloaderにRequestsを送り、Middlewareを通過させる。\n",
    "- 5.ページのダウンロードが完了すると、 Downloaderは、そのページのレスポンスを生成し、Scrapyエンジンに送信。 Middlewareを通過させる。\n",
    "- 6.Scrapyエンジンは、 Downloaderからレスポンスを受け取り、それをSpidersに送って処理する。Middlewareを通過させる。\n",
    "- 7.Spidersはレスポンスを処理し、スクレイピングされたItemと新しいRequestsをScrapyエンジンに返し、Middlewareを通過させる。\n",
    "- 8.Scrapyエンジンは処理済みのItemを Item・pipelineに送信し、処理済みのRequestsをSchedulerに送信し、可能なら次のクロールを行う。\n",
    "- 9.Schedulerからの要求がなくなるまで、プロセスは(ステップ1から)繰り返される。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
