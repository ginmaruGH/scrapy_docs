{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch.06 Scrapy Tutorial2\n",
    "\n",
    "<https://sugiaki1989.gitbook.io/scrapy-note/chapter06_tutorial02>\n",
    "\n",
    "[Books to scrape](http://books.toscrape.com/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## プロジェクトの作成\n",
    "\n",
    "```bash\n",
    "scrapy startproject sample_books\n",
    "\n",
    "cd sample_books\n",
    "\n",
    "scrapy genspider books_spider books.toscrape.com/\n",
    "```\n",
    "\n",
    "```bash\n",
    "tree\n",
    ".\n",
    "├── sample_books\n",
    "│   ├── __init__.py\n",
    "│   ├── __pycache__\n",
    "│   │   ├── __init__.cpython-310.pyc\n",
    "│   │   └── settings.cpython-310.pyc\n",
    "│   ├── items.py\n",
    "│   ├── middlewares.py\n",
    "│   ├── pipelines.py\n",
    "│   ├── settings.py\n",
    "│   └── spiders\n",
    "│       ├── __init__.py\n",
    "│       ├── __pycache__\n",
    "│       │   └── __init__.cpython-310.pyc\n",
    "│       └── books_spider.py\n",
    "└── scrapy.cfg\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML構造の調査とクローラーの設計\n",
    "\n",
    "- 各書籍のURLを取得\n",
    "- 詳細ページをスクレイピング\n",
    "- 次のページへ移動\n",
    "- 各書籍のURLを取得\n",
    "- 詳細ページをスクレイピング\n",
    "- ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 必要な項目のスクレイピング"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "scrapy shell \"http://books.toscrape.com\"  \n",
    "2023-06-30 20:25:22 [scrapy.utils.log] INFO: Scrapy 2.9.0 started (bot: sample_books)\n",
    "2023-06-30 20:25:22 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.10.11 (main, Apr 24 2023, 17:34:58) [Clang 14.0.3 (clang-1403.0.22.14.1)], pyOpenSSL 23.2.0 (OpenSSL 3.1.1 30 May 2023), cryptography 41.0.1, Platform macOS-13.4-x86_64-i386-64bit\n",
    "2023-06-30 20:25:22 [scrapy.crawler] INFO: Overridden settings:\n",
    "{'BOT_NAME': 'sample_books',\n",
    " 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter',\n",
    " 'FEED_EXPORT_ENCODING': 'utf-8',\n",
    " 'LOGSTATS_INTERVAL': 0,\n",
    " 'NEWSPIDER_MODULE': 'sample_books.spiders',\n",
    " 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n",
    " 'ROBOTSTXT_OBEY': True,\n",
    " 'SPIDER_MODULES': ['sample_books.spiders'],\n",
    " 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\n",
    "2023-06-30 20:25:22 [asyncio] DEBUG: Using selector: KqueueSelector\n",
    "2023-06-30 20:25:22 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
    "2023-06-30 20:25:22 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
    "2023-06-30 20:25:22 [scrapy.extensions.telnet] INFO: Telnet Password: 883ed7f4c4773ad9\n",
    "2023-06-30 20:25:22 [scrapy.middleware] INFO: Enabled extensions:\n",
    "['scrapy.extensions.corestats.CoreStats',\n",
    " 'scrapy.extensions.telnet.TelnetConsole',\n",
    " 'scrapy.extensions.memusage.MemoryUsage']\n",
    "2023-06-30 20:25:22 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
    "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
    " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
    " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
    " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
    " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
    " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
    " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
    " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
    " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
    " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
    " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
    " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
    "2023-06-30 20:25:22 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
    "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
    " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
    " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
    " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
    " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
    "2023-06-30 20:25:22 [scrapy.middleware] INFO: Enabled item pipelines:\n",
    "[]\n",
    "2023-06-30 20:25:22 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
    "2023-06-30 20:25:22 [scrapy.core.engine] INFO: Spider opened\n",
    "2023-06-30 20:25:23 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://books.toscrape.com/robots.txt> (referer: None)\n",
    "2023-06-30 20:25:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://books.toscrape.com> (referer: None)\n",
    "[s] Available Scrapy objects:\n",
    "[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)\n",
    "[s]   crawler    <scrapy.crawler.Crawler object at 0x10c5e7520>\n",
    "[s]   item       {}\n",
    "[s]   request    <GET http://books.toscrape.com>\n",
    "[s]   response   <200 http://books.toscrape.com>\n",
    "[s]   settings   <scrapy.settings.Settings object at 0x10c5e7130>\n",
    "[s]   spider     <BooksSpiderSpider 'books_spider' at 0x10cac2620>\n",
    "[s] Useful shortcuts:\n",
    "[s]   fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)\n",
    "[s]   fetch(req)                  Fetch a scrapy.Request and update local objects \n",
    "[s]   shelp()           Shell help (print this help)\n",
    "[s]   view(response)    View response in a browser\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    ">>> fetch(\"http://books.toscrape.com\")\n",
    "2023-06-30 20:26:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://books.toscrape.com> (referer: None)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ページ内の書籍のURLの一覧を取得\n",
    "\n",
    "```bash\n",
    ">>> response.xpath('//h3/a/@href').getall()\n",
    "['catalogue/a-light-in-the-attic_1000/index.html', 'catalogue/tipping-the-velvet_999/index.html', 'catalogue/soumission_998/index.html', 'catalogue/sharp-objects_997/index.html', 'catalogue/sapiens-a-brief-history-of-humankind_996/index.html', 'catalogue/the-requiem-red_995/index.html', 'catalogue/the-dirty-little-secrets-of-getting-your-dream-job_994/index.html', 'catalogue/the-coming-woman-a-novel-based-on-the-life-of-the-infamous-feminist-victoria-woodhull_993/index.html', 'catalogue/the-boys-in-the-boat-nine-americans-and-their-epic-quest-for-gold-at-the-1936-berlin-olympics_992/index.html', 'catalogue/the-black-maria_991/index.html', 'catalogue/starving-hearts-triangular-trade-trilogy-1_990/index.html', 'catalogue/shakespeares-sonnets_989/index.html', 'catalogue/set-me-free_988/index.html', 'catalogue/scott-pilgrims-precious-little-life-scott-pilgrim-1_987/index.html', 'catalogue/rip-it-up-and-start-again_986/index.html', 'catalogue/our-band-could-be-your-life-scenes-from-the-american-indie-underground-1981-1991_985/index.html', 'catalogue/olio_984/index.html', 'catalogue/mesaerion-the-best-science-fiction-stories-1800-1849_983/index.html', 'catalogue/libertarianism-for-beginners_982/index.html', 'catalogue/its-only-the-himalayas_981/index.html']\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URLのリストをfor-loopを使って、各書籍のページへリクエストを送る\n",
    "\n",
    "```python\n",
    "books = response.xpath('//h3/a/@href').getall()\n",
    "for book in books:\n",
    "     abs_url = response.urljoin(book)\n",
    "     yield Request(abs_url, callback=self.parse_book)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nextページに移動するためのURLを取得する\n",
    "\n",
    "```bash\n",
    ">>> response.xpath('//a[text()=\"next\"]/@href').get()\n",
    "'catalogue/page-2.html'\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 次のページへのURLを正規化してリクエスト送れるようにする\n",
    "\n",
    "```python\n",
    "next_page_url = response.xpath('//a[text()=\"next\"]/@href').get()\n",
    "abs_next_page_url = response.urljoin(next_page_url)\n",
    "if abs_next_page_url is not None:\n",
    "    yield Request(abs_next_page_url, callback=self.parse)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### parse関数\n",
    "\n",
    "```python\n",
    "def parse(self, response):\n",
    "    books = response.xpath('//h3/a/@href').getall()\n",
    "    for book in books:\n",
    "        abs_url = response.urljoin(book)\n",
    "        yield Request(abs_url, callback=self.parse_book)\n",
    "\n",
    "    # If there is a next button on this page, move the crawler\n",
    "    # このページに「Next」ボタンがある場合は、クローラーを移動させる。\n",
    "    next_page_url = response.xpath('//a[text()=\"next\"]/@href').get()\n",
    "    abs_next_page_url = response.urljoin(next_page_url)\n",
    "    if abs_next_page_url is not None:\n",
    "        yield Request(abs_next_page_url, callback=self.parse)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### spiders/books_spider.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### クローラーの実行\n",
    "\n",
    "```bash\n",
    "scrapy crawl books_spider -o result.json\n",
    "\n",
    "2023-06-30 21:06:12 [scrapy.core.engine] INFO: Closing spider (finished)\n",
    "2023-06-30 21:06:12 [scrapy.extensions.feedexport] INFO: Stored json feed (1000 items) in: result.json\n",
    "2023-06-30 21:06:12 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
    "{'downloader/request_bytes': 354623,\n",
    " 'downloader/request_count': 1051,\n",
    " 'downloader/request_method_count/GET': 1051,\n",
    " 'downloader/response_bytes': 22195383,\n",
    " 'downloader/response_count': 1051,\n",
    " 'downloader/response_status_count/200': 1050,\n",
    " 'downloader/response_status_count/404': 1,\n",
    " 'dupefilter/filtered': 1,\n",
    " 'elapsed_time_seconds': 47.900103,\n",
    " 'feedexport/success_count/FileFeedStorage': 1,\n",
    " 'finish_reason': 'finished',\n",
    " 'finish_time': datetime.datetime(2023, 6, 30, 12, 6, 12, 253752),\n",
    " 'item_scraped_count': 1000,\n",
    " 'log_count/DEBUG': 2055,\n",
    " 'log_count/INFO': 11,\n",
    " 'memusage/max': 57229312,\n",
    " 'memusage/startup': 57229312,\n",
    " 'request_depth_max': 50,\n",
    " 'response_received_count': 1051,\n",
    " 'robotstxt/request_count': 1,\n",
    " 'robotstxt/response_count': 1,\n",
    " 'robotstxt/response_status_count/404': 1,\n",
    " 'scheduler/dequeued': 1050,\n",
    " 'scheduler/dequeued/memory': 1050,\n",
    " 'scheduler/enqueued': 1050,\n",
    " 'scheduler/enqueued/memory': 1050,\n",
    " 'start_time': datetime.datetime(2023, 6, 30, 12, 5, 24, 353649)}\n",
    "2023-06-30 21:06:12 [scrapy.core.engine] INFO: Spider closed (finished)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
